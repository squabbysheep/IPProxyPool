# IP代理池

### 项目介绍

本项目通过爬虫抓取互联网上免费代理网站的IP，并且进行异步检测是否可用，如果可用就放入数据库。定时对数据库中的代理进行维护，然后通过web api的形式供外部使用。

### 代理池设计

```
本项目分为3个模块
# 第一个模块API模块：提供三个API
IP代理池首页 http://127.0.0.1:5000/
获取一个代理IP http://127.0.0.1:5000/get
获取一个池中现有ip数量 http://127.0.0.1:5000/count
# 第二个模块 ProxyPool代理池
mongodb封装 ：将代理池抽象成一个队列，从左边获取，从右边获取，放入队列，删除代理IP，清空队列，获取ip总数
代理ip的获取 ：使用元类，加入__SpiderFunc__和__SpiderFuncCount__两个参数，用来调用爬虫spider，请求抽出来，设置了请求头（可以替换），并做了请求失败的异常处理
# 第三个模块 Schedule调度器 
容量检查 ：判定代理池中代理ip数量，是否启动爬虫
定时检测代理 ：从数据库中拿出一半来检查，拿出来了（数据库就没有了），开始一部检测，成功则放回数据库
```


### 使用

```
>>> python run.py
```

启动成功，打开浏览器，[127.0.0.1:5000](http://127.0.0.1:5000/)  查看。

爬虫中获取代理：

```Python
import requests

def get_proxy():
    resp = requests.get('http://127.0.0.1:5000/get')
    proxy = resp.text
    ip = 'http://' + proxy
    return ip
```

### 注意

```
1.要先打开mongodb，不要忘记了
2.这里面的spider只有几个，需要你自己扩充
```

